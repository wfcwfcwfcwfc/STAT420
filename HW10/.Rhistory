red_mod_2 = step(pima_add_full, direction = "backward")
summary(red_mod_2)
pima_add_full_2 = glm(type ~ . ^ 2, data = Pima.tr, family = binomial)
pima_add_full_2
red_mod_2 = step(pima_add_full_2, direction = "backward")
summary(red_mod_2)
library(boot)
library(MASS)
set.seed(42)
# get cross-validated results for the polynomial model here
cv.glm(spam_trn, pima_glm, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the additive model here
cv.glm(spam_trn, pima_add_full, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the model selected from additive model here
cv.glm(spam_trn, red_mod, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the interaction model here
cv.glm(spam_trn, pima_add_full_2, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the model selected from interaction model here
cv.glm(spam_trn, red_mod_2, K = 5)$delta[1]
library(MASS)
library(boot)
# fit the models here
set.seed(42)
# get cross-validated results for the polynomial model here
cv.glm(Pima.tr, pima_glm, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the additive model here
cv.glm(Pima.tr, pima_add_full, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the model selected from additive model here
cv.glm(Pima.tr, red_mod, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the interaction model here
cv.glm(Pima.tr, pima_add_full_2, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the model selected from interaction model here
cv.glm(Pima.tr, red_mod_2, K = 5)$delta[1]
View(Pima.te)
mean(ifelse(predict(pima_add_full, newdata = Pima.te) > 0, "Yes", "No") != spam_trn$type)
mean(ifelse(predict(pima_add_full, newdata = Pima.te) > 0, "Yes", "No") != Pima.te$type)
make_conf_mat = function(predicted, actual) {
table(predicted = predicted, actual = actual)
}
pima_tst_pred = ifelse(predict(pima_add_full, Pima.te) > 0,
"spam",
"nonspam")
(conf_mat_50 = make_conf_mat(predicted = pima_tst_pred, actual = Pima.te$type))
pima_tst_pred = ifelse(predict(pima_add_full, Pima.te) > 0,
"Yes",
"No")
(conf_mat_50 = make_conf_mat(predicted = pima_tst_pred, actual = Pima.te$type))
66/109
pima_tst_pred_prob = ifelse(predict(pima_add_full, Pima.te, type = "response") > 0.3,
"Yes",
"No")
(conf_mat_50 = make_conf_mat(predicted = pima_tst_pred_prob, actual = Pima.te$type))
87/(22 + 87)
1-0.09534946
pima_add_full_2_both_reduced = step(pima_add_full_2, direction = "both")
summary(pima_add_full_2_both_reduced)
set.seed(42)
# get cross-validated results for the polynomial model here
cv.glm(Pima.tr, pima_glm, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the additive model here
cv.glm(Pima.tr, pima_add_full, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the model selected from additive model here
cv.glm(Pima.tr, red_mod, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the interaction model here
cv.glm(Pima.tr, pima_add_full_2, K = 5)$delta[1]
set.seed(42)
# get cross-validated results for the model selected from interaction model here
cv.glm(Pima.tr, pima_add_full_2_both_reduced, K = 5)$delta[1]
```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
sample_size = 150
set.seed(120)
x1 = rnorm(n = sample_size)
x2 = rnorm(n = sample_size)
x3 = rnorm(n = sample_size)
beta_0 = 0.4
beta_1 = -0.35
eta = beta_0 + beta_1 * x1
p = 1 / (1 + exp(-eta))
y = rbinom(n = sample_size, size = 1, prob = p)
data_1a = data.frame(y, x1, x2, x3)
fit_glm_1a = = glm(y ~ x1 + x2 + x3, data = data_1a, family = binomial)
beta_0 = 0.4
beta_1 = -0.35
eta = beta_0 + beta_1 * x1
p = 1 / (1 + exp(-eta))
y = rbinom(n = sample_size, size = 1, prob = p)
data_1a = data.frame(y, x1, x2, x3)
fit_glm_1a = glm(y ~ x1 + x2 + x3, data = data_1a, family = binomial)
summary(fit_glm_1a)
z_val = summary(fit_glm_1a)[3, 4]
z_val = coef(fit_glm_1a)[3, 4]
z_val = coef(fit_glm_1a)[3]
z_val
coef(fit_glm_1a)
summary(fit_glm_1a)[3]
summary(fit_glm_1a)
summary(fit_glm_1a).coef
summary(fit_glm_1a)[x2]
summary(fit_glm_1a)$coef
summary(fit_glm_1a)$coef['x2',]
summary(fit_glm_1a)$coef['x2','z value']
summary(fit_glm_1a)$coef['x2','z value']
fit_glm_null = glm(y ~ x1, data = data_1a, family = binomial)
anova(fit_glm_null, fit_glm_1a)
anova(fit_glm_null, fit_glm_1a, test = "LRT")
names(anova(fit_glm_null, fit_glm_1a, test = "LRT"))
anova(fit_glm_null, fit_glm_1a, test = "LRT")$Deviance
anova(fit_glm_null, fit_glm_1a, test = "LRT")$Deviance[2]
lrts[i] = anova(fit_glm_null, fit_glm_1a, test = "LRT")$Deviance[2]
beta_0 = 0.4
beta_1 = -0.35
sim_time = 2500
z_vals = rep(0, sim_time)
lrts = rep(0, sim_time)
for(i in 1:2500) {
eta = beta_0 + beta_1 * x1
p = 1 / (1 + exp(-eta))
y = rbinom(n = sample_size, size = 1, prob = p)
data_1a = data.frame(y, x1, x2, x3)
fit_glm_1a = glm(y ~ x1 + x2 + x3, data = data_1a, family = binomial)
fit_glm_null = glm(y ~ x1, data = data_1a, family = binomial)
z_vals[i] = summary(fit_glm_1a)$coef['x2','z value']
lrts[i] = anova(fit_glm_null, fit_glm_1a, test = "LRT")$Deviance[2]
}
hist(z_vals)
x_1b = seq(-4, 4, step = 0.1)
x_1b = seq(-4, 4, by = 0.1)
y_1b = pnorm(x_1b)
plot(x_1b, y_1b)
y_1b = dnorm(x_1b)
plot(x_1b, y_1b)
curve(x_1b, y_1b)
hist(z_vals)
x_1b = seq(-4, 4, by = 0.01)
y_1b = dnorm(x_1b)
curve(x_1b, y_1b)
x_1b = seq(-4, 4, by = 0.01)
y_1b = norm(x_1b)
y_1b = dnorm(x_1b)
curve(x_1b, y_1b)
hist(z_vals)
x_1b = seq(-4, 4, by = 0.01)
y_1b = dnorm(x_1b)
curve(x_1b, y_1b)
hist(z_vals, prob = TRUE)
y_1b = dnorm(x_1b, 0, 1)
hist(z_vals, prob = TRUE)
x_1b = seq(-4, 4, by = 0.01)
y_1b = dnorm(x_1b, 0, 1)
curve(x_1b, y_1b)
curve(y_1b)
hist(z_vals, prob = TRUE)
x_1b = seq(-4, 4, by = 0.01)
y_1b = dnorm(x_1b, 0, 1)
curve(dnorm(x_1b, 0, 1), col = "darkorange", add = TRUE, lwd = 3)
hist(z_vals, prob = TRUE)
x = seq(-4, 4, by = 0.01)
curve(dnorm(x, 0, 1), col = "darkorange", add = TRUE, lwd = 3)
hist(z_vals, prob = TRUE, breaks = 20, xlab = "z value", border = "dodgerblue")
x = seq(-4, 4, by = 0.01)
curve(dnorm(x, 0, 1), col = "darkorange", add = TRUE, lwd = 3)
sum(z_vals > 1) / length(z_vals)
pnorm(1, 0, 1, lower.tail = FALSE)
sum(z_vals > 1) / length(z_vals)
pnorm(1, 0, 1, lower.tail = FALSE)
hist(lrts, prob = TRUE, breaks = 20, xlab = "Likelihood Ratio Test Value", border = "dodgerblue")
hist(lrts, prob = TRUE, breaks = 20, xlab = "Likelihood Ratio Test Value", border = "dodgerblue")
x = seq(0, 20, by = 0.01)
curve(dchisq(x, 2, ncp = 0, log = FALSE), col = "darkorange", add = TRUE, lwd = 3)
sum(lrts > 5)
sum(lrts > 5) / length(lrts)
pchisq(5, 2, ncp = 0, lower.tail = FALSE)
sum(lrts > 5) / length(lrts)
pchisq(5, 2, ncp = 0, lower.tail = FALSE)
sample_size = 10
set.seed(120)
x1 = rnorm(n = sample_size)
x2 = rnorm(n = sample_size)
x3 = rnorm(n = sample_size)
beta_0 = 0.4
beta_1 = -0.35
sim_time = 2500
sample_size = 10
z_vals = rep(0, sim_time)
lrts = rep(0, sim_time)
for(i in 1:2500) {
eta = beta_0 + beta_1 * x1
p = 1 / (1 + exp(-eta))
y = rbinom(n = sample_size, size = 1, prob = p)
data_1a = data.frame(y, x1, x2, x3)
fit_glm_1a = glm(y ~ x1 + x2 + x3, data = data_1a, family = binomial)
fit_glm_null = glm(y ~ x1, data = data_1a, family = binomial)
z_vals[i] = summary(fit_glm_1a)$coef['x2','z value']
lrts[i] = anova(fit_glm_null, fit_glm_1a, test = "LRT")$Deviance[2]
}
hist(z_vals, prob = TRUE, breaks = 20, xlab = "z value", border
= "dodgerblue")
x = seq(-4, 4, by = 0.01)
curve(dnorm(x, 0, 1), col = "darkorange", add = TRUE, lwd = 3)
sum(z_vals > 1) / length(z_vals)
pnorm(1, 0, 1, lower.tail = FALSE)
hist(lrts, prob = TRUE, breaks = 20, xlab = "Likelihood Ratio Test Value",
border = "dodgerblue", main = "Histogram of LRTs")
x = seq(0, 20, by = 0.01)
curve(dchisq(x, 2, ncp = 0, log = FALSE), col = "darkorange", add = TRUE, lwd = 3)
sum(lrts > 5) / length(lrts)
pchisq(5, 2, ncp = 0, lower.tail = FALSE)
library(rpart.plot)
# install.packages("rpart")
# install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
data("ptitanic")
anova(fit_glm_null, fit_glm_1a, test = "LRT")$Deviance[2]
?rpart.plot::ptitanic
view(ptitanic_trn)
View(ptitanic_trn)
ptitanic = na.omit(ptitanic)
set.seed(2021)
trn_idx = sample(nrow(ptitanic), 300)
ptitanic_trn = ptitanic[trn_idx, ]
ptitanic_tst = ptitanic[-trn_idx, ]
View(ptitanic_trn)
ptitanic = na.omit(ptitanic)
set.seed(2021)
trn_idx = sample(nrow(ptitanic), 300)
ptitanic['2nd'] = ifelse(ptitanic['pclass'] == '2nd', 1, 0)
ptitanic['3rd'] = ifelse(ptitanic['pclass'] == '3rd', 1, 0)
ptitanic_trn = ptitanic[trn_idx, ]
ptitanic_tst = ptitanic[-trn_idx, ]
View(ptitanic)
View(ptitanic)
ptitanic = na.omit(ptitanic)
set.seed(2021)
trn_idx = sample(nrow(ptitanic), 300)
ptitanic['2nd'] = ifelse(ptitanic['pclass'] == '2nd', 1, 0)
ptitanic['3rd'] = ifelse(ptitanic['pclass'] == '3rd', 1, 0)
ptitanic['isMale'] = ifelse(ptitanic['sex'] == 'male', 1, 0)
ptitanic_trn = ptitanic[trn_idx, ]
ptitanic_tst = ptitanic[-trn_idx, ]
glm_2a = glm(survived ~ 2nd + 3rd + isMale + age + isMale:age, data = ptitanic_trn, family = binomial)
glm_2a = glm(survived ~ 2nd + '3rd' + isMale + age + isMale:age, data = ptitanic_trn, family = binomial)
glm_2a = glm(survived ~ 2nd + 3rd + isMale + age + isMale:age, data = ptitanic_trn, family = binomial)
ptitanic = na.omit(ptitanic)
set.seed(2021)
trn_idx = sample(nrow(ptitanic), 300)
ptitanic['second'] = ifelse(ptitanic['pclass'] == '2nd', 1, 0)
ptitanic['third'] = ifelse(ptitanic['pclass'] == '3rd', 1, 0)
ptitanic['isMale'] = ifelse(ptitanic['sex'] == 'male', 1, 0)
ptitanic_trn = ptitanic[trn_idx, ]
ptitanic_tst = ptitanic[-trn_idx, ]
glm_2a = glm(survived ~ second + third + isMale + age + isMale:age, data = ptitanic_trn, family = binomial)
summary(glm_2a)
glm_2b = glm(survived ~ isMale + age + isMale:age, data = ptitanic_trn, family = binomial)
anova(glm_2b, glm_2a, test = "LRT")
glm_3c = glm(survived ~ second + third + isMale + age , family = binomial)
glm_3c = glm(survived ~ second + third + isMale + age , data = ptitanic_trn, family = binomial)
anova(glm_3c, glm_3a, test = "LRT")
glm_2c = glm(survived ~ second + third + isMale + age , data = ptitanic_trn, family = binomial)
anova(glm_2c, glm_2a, test = "LRT")
make_conf_mat = function(predicted, actual) {
table(predicted = predicted, actual = actual)
}
pred_2d = predict(glm_2a, newdata = ptitanic_tst, type = "response")
make_conf_mat(pred_2d, ptitanic_tst$survived)
glm_2a
ptitanic$survived
pred_2d = ifelse(predict(glm_2a, newdata = ptitanic_tst, type = "response") > 0.5, "survived", "died")
make_conf_mat(pred_2d, ptitanic_tst$survived)
make_conf_mat = function(predicted, actual) {
table(predicted = predicted, actual = actual)
}
pred_2d = ifelse(predict(glm_2a, newdata = ptitanic_tst, type = "response") > 0.5, "survived", "died")
make_conf_mat(pred_2d, ptitanic_tst$survived)
mean(ifelse(pred_2d > 0.5, "survived", "died") != ptitanic_tst$survived)
make_conf_mat = function(predicted, actual) {
table(predicted = predicted, actual = actual)
}
pred_2d = ifelse(predict(glm_2a, newdata = ptitanic_tst, type = "response") > 0.5, "survived", "died")
conf_mat = make_conf_mat(pred_2d, ptitanic_tst$survived)
mis_cls_rate = mean(ifelse(pred_2d > 0.5, "survived", "died") != ptitanic_tst$survived)
sens = conf_mat[2, 2] / sum(conf_mat[, 2])
spec = conf_mat[1, 1] / sum(conf_mat[, 1])
wisc_train = read_csv("wisc-train.csv")
library(tidyverse)
library(readr)
wisc_train = read_csv("wisc-train.csv")
wisc_train = read_csv("wisc-train.csv")
View(wisc_train)
?wisc_train
wisc_train['class'] = as.factor(wisc_train['class'])
library(readr)
wisc_train = read_csv("wisc-train.csv")
wisc_train['class'] = as.factor(wisc_train['class'])
wisc_tst = read_csv("wisc-test.csv")
wisc_train
wisc_train = read.table("wisc-train.csv")
wisc_train = read.table("wisc-train.csv")
wisc_train$class = as.factor(wisc_train$class)
wisc_train$class = as.factor(wisc_train$class)
wisc_train = read_csv("wisc-train.csv")
wisc_train$class = as.factor(wisc_train$class)
wisc_train
glm_3a_1 = glm(class ~ radius + smoothness + texture, data = wisc_train, family = binomial)
glm_3a_full = glm(class ~ ., data = wisc_train, family = binomial)
glm_3a_back_aic = step(glm_3a_full, direction = "backward", trace = 0)
library(boot)
set.seed(42)
cv.glm(wisc_tst, glm_3a_1, K = 5)$delta[1]
wisc_train$mal = ifelse(wisc_train$class == 'B', 0, 1)
library(readr)
wisc_train = read_csv("wisc-train.csv")
wisc_train$class = as.factor(wisc_train$class)
wisc_train$mal = ifelse(wisc_train$class == 'B', 0, 1)
# why use '$' ?
wisc_tst = read_csv("wisc-test.csv")
wisc_tst$class = as.factor(wisc_train$class)
library(readr)
wisc_train = read_csv("wisc-train.csv")
wisc_train$class = as.factor(wisc_train$class)
wisc_train$mal = ifelse(wisc_train$class == 'B', 0, 1)
# why use '$' ?
wisc_tst = read_csv("wisc-test.csv")
wisc_tst$class = as.factor(wisc_tst$class)
wisc_tst$mal = ifelse(wisc_tst$class == 'B', 0, 1)
glm_3a_1 = glm(class ~ radius + smoothness + texture, data = wisc_train, family = binomial)
glm_3a_full = glm(class ~ ., data = wisc_train, family = binomial)
glm_3a_back_aic = step(glm_3a_full, direction = "backward", trace = 0)
library(boot)
set.seed(42)
cv.glm(wisc_tst, glm_3a_1, K = 5)$delta[1]
set.seed(42)
cv.glm(wisc_tst, glm_3a_full, K = 5)$delta[1]
set.seed(42)
cv.glm(wisc_tst, glm_3a_back_aic, K = 5)$delta[1]
cv.glm(wisc_tst, glm_3a_1, K = 5)$delta[1]
glm_3a_1 = glm(mal ~ radius + smoothness + texture, data = wisc_train, family = binomial)
glm_3a_full = glm(mal ~ ., data = wisc_train, family = binomial)
glm_3a_back_aic = step(glm_3a_full, direction = "backward", trace = 0)
library(boot)
set.seed(42)
cv.glm(wisc_tst, glm_3a_1, K = 5)$delta[1]
set.seed(42)
cv.glm(wisc_tst, glm_3a_full, K = 5)$delta[1]
set.seed(42)
cv.glm(wisc_tst, glm_3a_back_aic, K = 5)$delta[1]
glm_3a_1 = glm(mal ~ radius + smoothness + texture, data = wisc_train, family = binomial)
cv.glm(wisc_tst, glm_3a_1, K = 5)$delta[1]
wisc_tst
glm_3a_1 = glm(mal ~ radius + smoothness + texture, data = wisc_train, family = binomial)
glm_3a_full = glm(mal ~ . - class, data = wisc_train, family = binomial)
glm_3a_back_aic = step(glm_3a_full, direction = "backward", trace = 0)
library(boot)
set.seed(42)
cv.glm(wisc_tst, glm_3a_1, K = 5)$delta[1]
set.seed(42)
cv.glm(wisc_tst, glm_3a_full, K = 5)$delta[1]
set.seed(42)
cv.glm(wisc_tst, glm_3a_back_aic, K = 5)$delta[1]
glm_3a_1 = glm(mal ~ radius + smoothness + texture, data = wisc_train, family = binomial)
glm_3a_full = glm(mal ~ . - class, data = wisc_train, family = binomial)
glm_3a_back_aic = step(glm_3a_full, direction = "backward", trace = 0)
library(boot)
set.seed(42)
(cv.glm(wisc_tst, glm_3a_1, K = 5)$delta[1])
set.seed(42)
(cv.glm(wisc_tst, glm_3a_full, K = 5)$delta[1])
set.seed(42)
(cv.glm(wisc_tst, glm_3a_back_aic, K = 5)$delta[1])
glm_3a_1 = glm(class ~ radius + smoothness + texture, data = wisc_train, family = binomial)
(cv.glm(wisc_tst, glm_3a_1, K = 5)$delta[1])
cv.glm(wisc_train, glm_3a_1, K = 5)$delta[1]
glm_3a_1 = glm(class ~ radius + smoothness + texture, data = wisc_train, family = binomial)
glm_3a_full = glm(class ~ . - class, data = wisc_train, family = binomial)
glm_3a_back_aic = step(glm_3a_full, direction = "backward", trace = 0)
library(boot)
set.seed(42)
cv.glm(wisc_train, glm_3a_1, K = 5)$delta[1]
set.seed(42)
cv.glm(wisc_train, glm_3a_full, K = 5)$delta[1]
set.seed(42)
cv.glm(wisc_train, glm_3a_back_aic, K = 5)$delta[1]
glm_3a_1 = glm(class ~ radius + smoothness + texture, data = wisc_train, family = binomial)
glm_3a_full = glm(class ~ . - class, data = wisc_train, family = binomial)
glm_3a_back_aic = step(glm_3a_full, direction = "backward", trace = 0)
library(boot)
set.seed(42)
cv.glm(wisc_train, glm_3a_1, K = 5)$delta[1]
cv.glm(wisc_train, glm_3a_full, K = 5)$delta[1]
cv.glm(wisc_train, glm_3a_back_aic, K = 5)$delta[1]
glm_3a_1 = glm(class ~ radius + smoothness + texture, data = wisc_train, family = binomial)
glm_3a_full = glm(class ~ ., data = wisc_train, family = binomial)
glm_3a_back_aic = step(glm_3a_full, direction = "backward", trace = 0)
library(boot)
set.seed(42)
cv.glm(wisc_train, glm_3a_1, K = 5)$delta[1]
cv.glm(wisc_train, glm_3a_full, K = 5)$delta[1]
cv.glm(wisc_train, glm_3a_back_aic, K = 5)$delta[1]
glm_3a_2 = glm(class ~ . ^ 2, data = wisc_train, family = binomial)
glm_3a_back_aic = step(glm_3a_full, direction = "backward", trace = 0)
glm_3a_1 = glm(class ~ radius + smoothness + texture, data = wisc_train, family = binomial)
glm_3a_full = glm(class ~ ., data = wisc_train, family = binomial)
glm_3a_2 = glm(class ~ . ^ 2, data = wisc_train, family = binomial)
glm_3a_back_aic = step(glm_3a_full, direction = "backward", trace = 0)
library(boot)
set.seed(42)
cv.glm(wisc_train, glm_3a_1, K = 5)$delta[1]
cv.glm(wisc_train, glm_3a_full, K = 5)$delta[1]
cv.glm(wisc_train, glm_3a_back_aic, K = 5)$delta[1]
glm_3a_1 = glm(class ~ radius + smoothness + texture, data = wisc_train, family = binomial)
glm_3a_full = glm(class ~ ., data = wisc_train, family = binomial)
glm_3a_2 = glm(class ~ . ^ 2, data = wisc_train, family = binomial)
glm_3a_back_aic = step(glm_3a_2, direction = "backward", trace = 0)
library(boot)
set.seed(42)
cv.glm(wisc_train, glm_3a_1, K = 5)$delta[1]
cv.glm(wisc_train, glm_3a_full, K = 5)$delta[1]
cv.glm(wisc_train, glm_3a_back_aic, K = 5)$delta[1]
cv.glm(wisc_train, glm_3a_1, K = 5)$delta[1]
cv.glm(wisc_train, glm_3a_full, K = 5)$delta[1]
cv.glm(wisc_train, glm_3a_back_aic, K = 5)$delta[1]
cv.glm(wisc_train, glm_3a_back_aic, K = 5)
glm_3a_1 = glm(class ~ radius + smoothness + texture, data = wisc_train, family = binomial)
glm_3a_full = glm(class ~ ., data = wisc_train, family = binomial)
glm_3a_2 = glm(class ~ . ^ 2, data = wisc_train, family = binomial)
glm_3a_back_aic = step(glm_3a_2, direction = "backward", trace = 0)
library(boot)
set.seed(42)
cv.glm(wisc_train, glm_3a_1, K = 5)$delta[1]
cv.glm(wisc_train, glm_3a_full, K = 5)$delta[1]
cv.glm(wisc_train, glm_3a_back_aic, K = 5)$delta[1]
pred_3b = ifelse(predict(glm_3a_1, newdata = wisc_tst, type = "response") > 0.5, "M", "B")
conf_mat = make_conf_mat(pred_3b, wisc_tst$class)
sens = conf_mat[2, 2] / sum(conf_mat[, 2])
spec = conf_mat[1, 1] / sum(conf_mat[, 1])
l = length(cutoffs)
cutoffs = seq(0.01, 0.99, by = 0.01)
l = length(cutoffs)
sensl = rep(l)
specl = rep(l)
for(i in l) {
pred_3b = ifelse(predict(glm_3a_1, newdata = wisc_tst, type = "response") > cutoffs[l], "M", "B")
conf_mat = make_conf_mat(pred_3b, wisc_tst$class)
sensl[i] = conf_mat[2, 2] / sum(conf_mat[, 2])
spec[i] = conf_mat[1, 1] / sum(conf_mat[, 1])
}
plot(x, sens)
plot(cutoffs, sens)
plot(cutoffs, sensl)
l = length(cutoffs)
sensl = rep(l)
specl = rep(l)
for(i in l) {
pred_3b = ifelse(predict(glm_3a_1, newdata = wisc_tst, type = "response") > cutoffs[l], "M", "B")
conf_mat = make_conf_mat(pred_3b, wisc_tst$class)
sensl[i] = conf_mat[2, 2] / sum(conf_mat[, 2])
specl[i] = conf_mat[1, 1] / sum(conf_mat[, 1])
}
plot(cutoffs, sensl)
l = length(cutoffs)
sensl = rep(l)
specl = rep(l)
for(i in l) {
pred_3b = ifelse(predict(glm_3a_1, newdata = wisc_tst, type = "response") > cutoffs[i], "M", "B")
conf_mat = make_conf_mat(pred_3b, wisc_tst$class)
sensl[i] = conf_mat[2, 2] / sum(conf_mat[, 2])
specl[i] = conf_mat[1, 1] / sum(conf_mat[, 1])
}
plot(cutoffs, sensl)
curve(specl[cutoffs])
l = length(cutoffs)
sensl = rep(l)
specl = rep(l)
for(i in 1:l) {
pred_3b = ifelse(predict(glm_3a_1, newdata = wisc_tst, type = "response") > cutoffs[i], "M", "B")
conf_mat = make_conf_mat(pred_3b, wisc_tst$class)
sensl[i] = conf_mat[2, 2] / sum(conf_mat[, 2])
specl[i] = conf_mat[1, 1] / sum(conf_mat[, 1])
}
plot(cutoffs, sensl)
curve(specl[cutoffs])
plot(cutoffs, sensl)
x = cutoffs
curve(specl[x])
)
plot(cutoffs, sensl)
plot(cutoffs, specl)
plot(cutoffs, sensl)
curve(cutoffs, specl)
l = length(cutoffs)
sensl = rep(l)
specl = rep(l)
for(i in 1:l) {
pred_3b = ifelse(predict(glm_3a_1, newdata = wisc_tst, type = "response") > cutoffs[i], "M", "B")
conf_mat = make_conf_mat(pred_3b, wisc_tst$class)
sensl[i] = conf_mat[2, 2] / sum(conf_mat[, 2])
specl[i] = conf_mat[1, 1] / sum(conf_mat[, 1])
}
plot(cutoffs, sensl)
lines(cutoffs, specl)
